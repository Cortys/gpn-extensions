{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 03:34:26.645157: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-05 03:34:26.836610: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-05 03:34:26.836640: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-05 03:34:26.837779: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-05 03:34:26.931911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-05 03:34:27.794679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from gpn.data.dataset_manager import DatasetManager\n",
    "from gpn.data.dataset_provider import InMemoryDatasetProvider\n",
    "\n",
    "\n",
    "dataset = \"ogbn-arxiv\"\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    dataset_provider = InMemoryDatasetProvider(\n",
    "        DatasetManager(\n",
    "            dataset=dataset,\n",
    "            split_no=1,\n",
    "            root=\"./data\",\n",
    "            ood_flag=False,\n",
    "            train_samples_per_class=0.05,\n",
    "            val_samples_per_class=0.15,\n",
    "            test_samples_per_class=0.8,\n",
    "            split=\"public\" if dataset == \"ogbn-arxiv\" else \"random\",\n",
    "            # ood_setting=\"poisoning\",\n",
    "            # ood_type=\"leave_out_classes\",\n",
    "            # ood_num_left_out_classes=-1,\n",
    "            # ood_leave_out_last_classes=True,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return dataset_provider\n",
    "\n",
    "data = load_dataset(dataset).data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - train_and_eval - No observers have been added to this run\n",
      "INFO - train_and_eval - Running command 'run_experiment'\n",
      "INFO - train_and_eval - Started\n",
      "INFO - root - Received the following configuration:\n",
      "INFO - root - RUN\n",
      "INFO - root - {'experiment_name': 'classification', 'experiment_directory': './saved_experiments', 'reduced_training_metrics': False, 'eval_mode': 'default', 'job': 'predict', 'save_model': True, 'gpu': 0, 'num_inits': 1, 'num_splits': 1, 'log': False, 'debug': True, 'ex_type': 'transductive', 'ood_loc': True, 'ood_loc_only': False, 'ood_edge_perturbations': True, 'ood_isolated_perturbations': False}\n",
      "INFO - root - -----------------------------------------\n",
      "INFO - root - DATA\n",
      "INFO - root - {'to_sparse': False, 'split_no': 1, 'dataset': 'ogbn-arxiv', 'root': './data', 'split': 'public', 'train_samples_per_class': 0.05, 'val_samples_per_class': 0.15, 'test_samples_per_class': 0.8, 'ood_flag': False}\n",
      "INFO - root - -----------------------------------------\n",
      "INFO - root - MODEL\n",
      "INFO - root - {'model_name': 'GPN', 'seed': 42, 'init_no': 1, 'dim_hidden': 64, 'dropout_prob': 0.5, 'dropout_prob_adj': 0.0, 'entropy_num_samples': 100, 'K': 10, 'alpha_teleport': 0.1, 'add_self_loops': True, 'adj_normalization': 'rw', 'sparse_propagation': True, 'radial_layers': 10, 'maf_layers': 0, 'gaussian_layers': 0, 'dim_latent': 16, 'alpha_evidence_scale': 'latent-new', 'entropy_reg': 0.0001, 'flow_weight_decay': 0.0, 'use_batched_flow': True, 'pre_train_mode': 'flow', 'approximate_reg': True, 'loss_reduction': 'sum'}\n",
      "INFO - root - -----------------------------------------\n",
      "INFO - root - TRAINING\n",
      "INFO - root - {'lr': 0.01, 'weight_decay': 0.001, 'epochs': 100000, 'warmup_epochs': 5, 'finetune_epochs': 0, 'stopping_mode': 'default', 'stopping_patience': 50, 'stopping_restore_best': True, 'stopping_metric': 'val_CE', 'stopping_minimize': True}\n",
      "INFO - root - -----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment (model=GPN, dataset=ogbn-arxiv, ood_type=None, split=1, init=1, results=./saved_experiments/classification/GPN/158/results_1.json, trained=False, evaluated=False).\n",
      "Completed experiment (model=GPN, dataset=ogbn-arxiv, ood_type=None, split=1, init=1, results=./saved_experiments/classification/GPN/158/results_1.json, trained=False, evaluated=False).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - train_and_eval - Result: [Prediction(soft=tensor([[1.3872e-05, 3.7093e-03, 1.0660e-03,  ..., 2.1653e-05, 1.0974e-05,\n",
      "         3.6581e-03],\n",
      "        [2.1655e-04, 3.7456e-03, 1.2107e-03,  ..., 2.2383e-04, 2.1380e-04,\n",
      "         3.6561e-03],\n",
      "        [1.0802e-04, 3.8549e-03, 1.1764e-03,  ..., 1.1589e-04, 1.0515e-04,\n",
      "         3.7669e-03],\n",
      "        ...,\n",
      "        [1.3598e-04, 3.9237e-03, 1.2146e-03,  ..., 1.4381e-04, 1.3314e-04,\n",
      "         3.8045e-03],\n",
      "        [1.4829e-04, 3.8305e-03, 1.2069e-03,  ..., 1.5613e-04, 1.4538e-04,\n",
      "         3.7804e-03],\n",
      "        [2.5477e-04, 3.7782e-03, 1.2481e-03,  ..., 2.6288e-04, 2.5188e-04,\n",
      "         3.8722e-03]]), log_soft=tensor([[-11.1857,  -5.5969,  -6.8439,  ..., -10.7404, -11.4200,  -5.6108],\n",
      "        [ -8.4377,  -5.5872,  -6.7165,  ...,  -8.4046,  -8.4505,  -5.6114],\n",
      "        [ -9.1332,  -5.5584,  -6.7453,  ...,  -9.0629,  -9.1601,  -5.5815],\n",
      "        ...,\n",
      "        [ -8.9030,  -5.5407,  -6.7133,  ...,  -8.8470,  -8.9241,  -5.5716],\n",
      "        [ -8.8163,  -5.5648,  -6.7197,  ...,  -8.7648,  -8.8361,  -5.5779],\n",
      "        [ -8.2752,  -5.5785,  -6.6862,  ...,  -8.2438,  -8.2866,  -5.5539]]), hard=tensor([31, 31, 31,  ..., 31, 31, 31]), alpha=tensor([[  1.3998, 374.3158, 107.5700,  ...,   2.1851,   1.1074, 369.1520],\n",
      "        [  1.0176,  17.6008,   5.6893,  ...,   1.0518,   1.0047,  17.1803],\n",
      "        [  1.0381,  37.0455,  11.3051,  ...,   1.1137,   1.0105,  36.1996],\n",
      "        ...,\n",
      "        [  1.0296,  29.7073,   9.1962,  ...,   1.0888,   1.0080,  28.8047],\n",
      "        [  1.0276,  26.5435,   8.3633,  ...,   1.0819,   1.0074,  26.1965],\n",
      "        [  1.0157,  15.0630,   4.9758,  ...,   1.0481,   1.0042,  15.4378]]), alpha_features=None, propagation_weights=None, x_hat=None, logits=None, logits_features=None, latent=tensor([[-0.0525, -0.0546, -0.1041,  ..., -0.0204,  0.0694,  0.0674],\n",
      "        [-0.0222, -0.0473, -0.1269,  ..., -0.0308,  0.0530,  0.0519],\n",
      "        [-0.0453, -0.1039, -0.1048,  ..., -0.0651,  0.0860,  0.0876],\n",
      "        ...,\n",
      "        [-0.0870, -0.0587, -0.1079,  ..., -0.0093,  0.0619,  0.0655],\n",
      "        [-0.0359, -0.0505, -0.1073,  ...,  0.0027,  0.0895,  0.0740],\n",
      "        [-0.0368, -0.0774, -0.1011,  ...,  0.0320,  0.0453,  0.0928]]), latent_node=None, latent_features=tensor([[-0.0525, -0.0546, -0.1041,  ..., -0.0204,  0.0694,  0.0674],\n",
      "        [-0.0222, -0.0473, -0.1269,  ..., -0.0308,  0.0530,  0.0519],\n",
      "        [-0.0453, -0.1039, -0.1048,  ..., -0.0651,  0.0860,  0.0876],\n",
      "        ...,\n",
      "        [-0.0870, -0.0587, -0.1079,  ..., -0.0093,  0.0619,  0.0655],\n",
      "        [-0.0359, -0.0505, -0.1073,  ...,  0.0027,  0.0895,  0.0740],\n",
      "        [-0.0368, -0.0774, -0.1011,  ...,  0.0320,  0.0453,  0.0928]]), hidden=tensor([[0.0000, 0.0000, 0.0085,  ..., 0.2185, 0.0000, 0.2339],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3736, 0.0000, 0.2302],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1682, 0.0000, 0.2854],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1844, 0.0000, 0.2569],\n",
      "        [0.0000, 0.0000, 0.0301,  ..., 0.2920, 0.0000, 0.1773],\n",
      "        [0.0000, 0.0000, 0.0278,  ..., 0.2484, 0.0000, 0.1061]]), hidden_features=tensor([[0.0000, 0.0000, 0.0085,  ..., 0.2185, 0.0000, 0.2339],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3736, 0.0000, 0.2302],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1682, 0.0000, 0.2854],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1844, 0.0000, 0.2569],\n",
      "        [0.0000, 0.0000, 0.0301,  ..., 0.2920, 0.0000, 0.1773],\n",
      "        [0.0000, 0.0000, 0.0278,  ..., 0.2484, 0.0000, 0.1061]]), var_predicted=None, var=None, softs=None, energy=None, energy_feaures=None, p_c=None, p_u=None, p_uc=None, chi=None, evidence=tensor([100873.3672,   4659.0776,   9569.9014,  ...,   7531.1582,\n",
      "          6889.5430,   3946.8210]), evidence_ft=tensor([9172.4453, 8988.8105, 9312.0059,  ..., 7963.3613, 8249.2764,\n",
      "        8332.7871]), evidence_nn=None, evidence_node=None, evidence_per_class=None, evidence_ft_per_class=None, ft_weight=None, nn_weight=None, log_ft=None, log_ft_per_class=tensor([[-23.6213, -16.7450, -18.0835,  ..., -22.5650, -24.9872, -16.7389],\n",
      "        [-23.6343, -16.8080, -18.0731,  ..., -22.5573, -24.9578, -16.8174],\n",
      "        [-23.6086, -16.7854, -18.0300,  ..., -22.6008, -24.9225, -16.8093],\n",
      "        ...,\n",
      "        [-23.7129, -16.7487, -17.9942,  ..., -22.5268, -24.9800, -16.8086],\n",
      "        [-23.6046, -16.8797, -18.0871,  ..., -22.5101, -24.9293, -16.8534],\n",
      "        [-23.5929, -16.8872, -18.1210,  ..., -22.4170, -24.9302, -16.8082]]), log_nn=None, log_nn_per_class=None, log_node=None, prediction_confidence_aleatoric=tensor([0.5994, 0.6061, 0.6070,  ..., 0.6010, 0.5962, 0.5786]), prediction_confidence_epistemic=tensor([60485.7891,  2848.0933,  5832.8770,  ...,  4550.2080,  4131.6235,\n",
      "         2306.5999]), prediction_confidence_structure=None, sample_confidence_total=tensor([0.5994, 0.6061, 0.6070,  ..., 0.6010, 0.5962, 0.5786]), sample_confidence_total_entropy=tensor([-1.3337, -1.3607, -1.3411,  ..., -1.3602, -1.3674, -1.4214]), sample_confidence_aleatoric=tensor([0.5994, 0.6061, 0.6070,  ..., 0.6010, 0.5962, 0.5786]), sample_confidence_aleatoric_entropy=tensor([-1.3342, -1.3589, -1.3390,  ..., -1.3552, -1.3651, -1.4179]), sample_confidence_epistemic=tensor([100913.3672,   4699.0776,   9609.9014,  ...,   7571.1582,\n",
      "          6929.5430,   3986.8210]), sample_confidence_epistemic_entropy=tensor([275.2656, 274.5938, 276.1016,  ..., 270.1328, 271.6289, 271.8633]), sample_confidence_epistemic_entropy_diff=tensor([ 0.0006, -0.0018, -0.0021,  ..., -0.0050, -0.0023, -0.0035]), sample_confidence_structure=None, sample_confidence_features=tensor([9212.4453, 9028.8105, 9352.0059,  ..., 8003.3613, 8289.2764,\n",
      "        8372.7871]), sample_confidence_neighborhood=None, mu_1=None, mu_1p=None, mu_2=None, mu_2p=None, var_1=None, var_1p=None, var_2=None, var_2p=None, log_q=None, log_prior=None, act_vec=None, q=None)]\n",
      "INFO - train_and_eval - Completed after 0:00:14\n",
      "WARNING - train_and_eval - No observers have been added to this run\n",
      "INFO - train_and_eval - Running command 'run_experiment'\n",
      "INFO - train_and_eval - Started\n",
      "INFO - root - Received the following configuration:\n",
      "INFO - root - RUN\n",
      "INFO - root - {'experiment_name': 'classification', 'experiment_directory': './saved_experiments', 'reduced_training_metrics': False, 'eval_mode': 'default', 'job': 'predict', 'save_model': True, 'gpu': 0, 'num_inits': 1, 'num_splits': 1, 'log': False, 'debug': True, 'ex_type': 'transductive', 'ood_loc': True, 'ood_loc_only': False, 'ood_edge_perturbations': True, 'ood_isolated_perturbations': False}\n",
      "INFO - root - -----------------------------------------\n",
      "INFO - root - DATA\n",
      "INFO - root - {'to_sparse': False, 'split_no': 1, 'dataset': 'ogbn-arxiv', 'root': './data', 'split': 'public', 'train_samples_per_class': 0.05, 'val_samples_per_class': 0.15, 'test_samples_per_class': 0.8, 'ood_flag': False}\n",
      "INFO - root - -----------------------------------------\n",
      "INFO - root - MODEL\n",
      "INFO - root - {'model_name': 'GPN_LOP', 'seed': 42, 'init_no': 1, 'dim_hidden': 64, 'dropout_prob': 0.5, 'dropout_prob_adj': 0.0, 'entropy_num_samples': 100, 'K': 10, 'alpha_teleport': 0.1, 'add_self_loops': True, 'sparse_propagation': True, 'sparse_x_prune_threshold': 0.01, 'radial_layers': 10, 'maf_layers': 0, 'gaussian_layers': 0, 'dim_latent': 16, 'alpha_evidence_scale': 'latent-new', 'entropy_reg': 0.0001, 'flow_weight_decay': 0.0, 'use_batched_flow': True, 'pre_train_mode': 'flow', 'approximate_reg': True, 'loss_reduction': 'sum'}\n",
      "INFO - root - -----------------------------------------\n",
      "INFO - root - TRAINING\n",
      "INFO - root - {'lr': 0.01, 'weight_decay': 0.001, 'epochs': 100000, 'warmup_epochs': 5, 'finetune_epochs': 0, 'stopping_mode': 'default', 'stopping_patience': 50, 'stopping_restore_best': True, 'stopping_metric': 'val_CE', 'stopping_minimize': True}\n",
      "INFO - root - -----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment (model=GPN_LOP, dataset=ogbn-arxiv, ood_type=None, split=1, init=1, results=./saved_experiments/classification/GPN_LOP/51/results_1.json, trained=True, evaluated=False).\n",
      "Completed experiment (model=GPN_LOP, dataset=ogbn-arxiv, ood_type=None, split=1, init=1, results=./saved_experiments/classification/GPN_LOP/51/results_1.json, trained=True, evaluated=False).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - train_and_eval - Result: [Prediction(soft=tensor([[0.0015, 0.0034, 0.0158,  ..., 0.0225, 0.0006, 0.0031],\n",
      "        [0.0064, 0.0013, 0.0212,  ..., 0.0080, 0.0006, 0.0353],\n",
      "        [0.0133, 0.0056, 0.0083,  ..., 0.0115, 0.0013, 0.0146],\n",
      "        ...,\n",
      "        [0.0021, 0.0024, 0.0389,  ..., 0.0144, 0.0030, 0.0036],\n",
      "        [0.0048, 0.0058, 0.0143,  ..., 0.0132, 0.0014, 0.0049],\n",
      "        [0.0004, 0.0200, 0.0027,  ..., 0.0138, 0.0024, 0.0030]]), log_soft=tensor([[-6.5122, -5.6720, -4.1480,  ..., -3.7955, -7.4374, -5.7905],\n",
      "        [-5.0439, -6.6765, -3.8548,  ..., -4.8302, -7.4524, -3.3450],\n",
      "        [-4.3188, -5.1791, -4.7956,  ..., -4.4613, -6.6256, -4.2238],\n",
      "        ...,\n",
      "        [-6.1626, -6.0326, -3.2457,  ..., -4.2374, -5.7937, -5.6163],\n",
      "        [-5.3321, -5.1470, -4.2494,  ..., -4.3265, -6.5636, -5.3211],\n",
      "        [-7.7787, -3.9105, -5.9155,  ..., -4.2805, -6.0250, -5.8014]]), hard=tensor([ 4, 28, 28,  ..., 30, 16,  8]), alpha=tensor([[ 12.0600,  41.1330, 144.2998,  ..., 207.7222,   5.1618,  26.7567],\n",
      "        [ 64.9457,  18.8078, 231.5927,  ...,  78.1301,   7.3954, 353.4913],\n",
      "        [120.6912,  48.4501,  61.2112,  ...,  70.0430,   8.1833, 135.6071],\n",
      "        ...,\n",
      "        [ 17.8262,  27.4703, 428.1812,  ..., 146.6792,  33.2783,  36.1252],\n",
      "        [ 28.1693, 103.8123,  96.1166,  ...,  98.5302,  13.0356,  30.7477],\n",
      "        [  4.3463, 269.3209,  34.4859,  ..., 177.6670,  28.4013,  33.0118]]), alpha_features=tensor([[  8.9877,  28.3316, 166.8410,  ..., 246.1647,   3.4055,  26.4599],\n",
      "        [ 50.4813,   8.0834, 163.4983,  ..., 104.8604,   8.0276, 271.2108],\n",
      "        [ 34.8223,  37.2630,  60.9588,  ..., 122.1177,  11.7488,  79.0819],\n",
      "        ...,\n",
      "        [ 11.1486,  12.9704, 214.5630,  ...,  94.6118,  18.6617,  20.1257],\n",
      "        [  8.9033,  17.2468, 196.4217,  ..., 124.9403,   2.6188,  39.2169],\n",
      "        [  4.6111, 348.1633,  50.4970,  ..., 240.4626,  12.3002,  17.8109]]), propagation_weights=SparseTensor(row=tensor([     0,      0,      0,  ..., 169342, 169342, 169342]),\n",
      "             col=tensor([     0,  30038,  59944,  ...,  96859, 158981, 169342]),\n",
      "             val=tensor([0.7762, 0.0135, 0.0122,  ..., 0.0255, 0.3191, 0.2222]),\n",
      "             size=(169343, 169343), nnz=2723277, density=0.01%), x_hat=None, logits=None, logits_features=None, latent=tensor([[ 0.0146,  0.0698, -0.1315,  ..., -0.0511, -0.1003,  0.1225],\n",
      "        [ 0.1629, -0.0010,  0.0820,  ..., -0.0905, -0.1064,  0.0186],\n",
      "        [ 0.0720, -0.0055, -0.0464,  ..., -0.0859, -0.0278, -0.0082],\n",
      "        ...,\n",
      "        [-0.0531, -0.0876,  0.0416,  ...,  0.0261,  0.0063,  0.0457],\n",
      "        [ 0.0481,  0.0169, -0.0269,  ..., -0.0800, -0.0707,  0.0909],\n",
      "        [ 0.2491,  0.1819, -0.3114,  ..., -0.0774,  0.0026, -0.0010]]), latent_node=None, latent_features=tensor([[ 0.0146,  0.0698, -0.1315,  ..., -0.0511, -0.1003,  0.1225],\n",
      "        [ 0.1629, -0.0010,  0.0820,  ..., -0.0905, -0.1064,  0.0186],\n",
      "        [ 0.0720, -0.0055, -0.0464,  ..., -0.0859, -0.0278, -0.0082],\n",
      "        ...,\n",
      "        [-0.0531, -0.0876,  0.0416,  ...,  0.0261,  0.0063,  0.0457],\n",
      "        [ 0.0481,  0.0169, -0.0269,  ..., -0.0800, -0.0707,  0.0909],\n",
      "        [ 0.2491,  0.1819, -0.3114,  ..., -0.0774,  0.0026, -0.0010]]), hidden=tensor([[0.4267, 0.0330, 0.0000,  ..., 0.1862, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0700, 0.0000, 0.0000,  ..., 0.0987, 0.0000, 0.0634],\n",
      "        ...,\n",
      "        [0.0837, 0.0676, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3752, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],\n",
      "        [0.1621, 0.0000, 0.0000,  ..., 0.4673, 0.0000, 0.0000]]), hidden_features=tensor([[0.4267, 0.0330, 0.0000,  ..., 0.1862, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0700, 0.0000, 0.0000,  ..., 0.0987, 0.0000, 0.0634],\n",
      "        ...,\n",
      "        [0.0837, 0.0676, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3752, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],\n",
      "        [0.1621, 0.0000, 0.0000,  ..., 0.4673, 0.0000, 0.0000]]), var_predicted=None, var=None, softs=None, energy=None, energy_feaures=None, p_c=None, p_u=None, p_uc=None, chi=None, evidence=tensor([[ 9729.3281],\n",
      "        [14745.5352],\n",
      "        [ 9450.3203],\n",
      "        ...,\n",
      "        [11008.9746],\n",
      "        [10572.5801],\n",
      "        [13348.4492]]), evidence_ft=tensor([ 9203.0605,  9049.6455,  5164.0967,  ...,  9245.5527,  6546.1890,\n",
      "        22053.8418]), evidence_nn=None, evidence_node=None, evidence_per_class=None, evidence_ft_per_class=None, ft_weight=None, nn_weight=None, log_ft=None, log_ft_per_class=tensor([[-18.1703, -16.9401, -15.1372,  ..., -14.7463, -19.3704, -17.0111],\n",
      "        [-16.3466, -18.2904, -15.1575,  ..., -15.6051, -18.2983, -14.6490],\n",
      "        [-16.7271, -16.6574, -16.1545,  ..., -15.4514, -17.8734, -15.8904],\n",
      "        ...,\n",
      "        [-17.9309, -17.7658, -14.8843,  ..., -15.7090, -17.3768, -17.2972],\n",
      "        [-18.1809, -17.4603, -14.9730,  ..., -15.4284, -19.7665, -16.6049],\n",
      "        [-18.9642, -14.3984, -16.3463,  ..., -14.7698, -17.8234, -17.4262]]), log_nn=None, log_nn_per_class=None, log_node=None, prediction_confidence_aleatoric=tensor([0.3434, 0.4361, 0.4080,  ..., 0.2437, 0.2019, 0.4793]), prediction_confidence_epistemic=tensor([3258.5999, 8635.1875, 4977.8286,  ..., 3080.4431, 4455.8511,\n",
      "        6588.4492]), prediction_confidence_structure=None, sample_confidence_total=tensor([0.3434, 0.4361, 0.4080,  ..., 0.2437, 0.2019, 0.4793]), sample_confidence_total_entropy=tensor([-2.6345, -2.2541, -2.4566,  ..., -2.5333, -2.7752, -2.1102]), sample_confidence_aleatoric=tensor([0.3434, 0.4361, 0.4080,  ..., 0.2437, 0.2019, 0.4793]), sample_confidence_aleatoric_entropy=tensor([-2.3891, -1.9910, -2.1478,  ..., -2.2385, -2.2826, -1.9910]), sample_confidence_epistemic=tensor([ 9769.3291, 14785.5361,  9490.3213,  ..., 11048.9746, 10612.5801,\n",
      "        13388.4473]), sample_confidence_epistemic_entropy=tensor([226.4146, 240.0626, 224.8981,  ..., 229.2888, 223.2633, 240.3971]), sample_confidence_epistemic_entropy_diff=tensor([-0.2454, -0.2631, -0.3088,  ..., -0.2948, -0.4926, -0.1192]), sample_confidence_structure=None, sample_confidence_features=tensor([ 9243.0605,  9089.6455,  5204.0967,  ...,  9285.5527,  6586.1890,\n",
      "        22093.8418]), sample_confidence_neighborhood=None, mu_1=None, mu_1p=None, mu_2=None, mu_2p=None, var_1=None, var_1p=None, var_2=None, var_2p=None, log_q=None, log_prior=None, act_vec=None, q=None)]\n",
      "INFO - train_and_eval - Completed after 0:00:35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(soft=tensor([[0.0015, 0.0034, 0.0158,  ..., 0.0225, 0.0006, 0.0031],\n",
       "        [0.0064, 0.0013, 0.0212,  ..., 0.0080, 0.0006, 0.0353],\n",
       "        [0.0133, 0.0056, 0.0083,  ..., 0.0115, 0.0013, 0.0146],\n",
       "        ...,\n",
       "        [0.0021, 0.0024, 0.0389,  ..., 0.0144, 0.0030, 0.0036],\n",
       "        [0.0048, 0.0058, 0.0143,  ..., 0.0132, 0.0014, 0.0049],\n",
       "        [0.0004, 0.0200, 0.0027,  ..., 0.0138, 0.0024, 0.0030]]), log_soft=tensor([[-6.5122, -5.6720, -4.1480,  ..., -3.7955, -7.4374, -5.7905],\n",
       "        [-5.0439, -6.6765, -3.8548,  ..., -4.8302, -7.4524, -3.3450],\n",
       "        [-4.3188, -5.1791, -4.7956,  ..., -4.4613, -6.6256, -4.2238],\n",
       "        ...,\n",
       "        [-6.1626, -6.0326, -3.2457,  ..., -4.2374, -5.7937, -5.6163],\n",
       "        [-5.3321, -5.1470, -4.2494,  ..., -4.3265, -6.5636, -5.3211],\n",
       "        [-7.7787, -3.9105, -5.9155,  ..., -4.2805, -6.0250, -5.8014]]), hard=tensor([ 4, 28, 28,  ..., 30, 16,  8]), alpha=tensor([[ 12.0600,  41.1330, 144.2998,  ..., 207.7222,   5.1618,  26.7567],\n",
       "        [ 64.9457,  18.8078, 231.5927,  ...,  78.1301,   7.3954, 353.4913],\n",
       "        [120.6912,  48.4501,  61.2112,  ...,  70.0430,   8.1833, 135.6071],\n",
       "        ...,\n",
       "        [ 17.8262,  27.4703, 428.1812,  ..., 146.6792,  33.2783,  36.1252],\n",
       "        [ 28.1693, 103.8123,  96.1166,  ...,  98.5302,  13.0356,  30.7477],\n",
       "        [  4.3463, 269.3209,  34.4859,  ..., 177.6670,  28.4013,  33.0118]]), alpha_features=tensor([[  8.9877,  28.3316, 166.8410,  ..., 246.1647,   3.4055,  26.4599],\n",
       "        [ 50.4813,   8.0834, 163.4983,  ..., 104.8604,   8.0276, 271.2108],\n",
       "        [ 34.8223,  37.2630,  60.9588,  ..., 122.1177,  11.7488,  79.0819],\n",
       "        ...,\n",
       "        [ 11.1486,  12.9704, 214.5630,  ...,  94.6118,  18.6617,  20.1257],\n",
       "        [  8.9033,  17.2468, 196.4217,  ..., 124.9403,   2.6188,  39.2169],\n",
       "        [  4.6111, 348.1633,  50.4970,  ..., 240.4626,  12.3002,  17.8109]]), propagation_weights=SparseTensor(row=tensor([     0,      0,      0,  ..., 169342, 169342, 169342]),\n",
       "             col=tensor([     0,  30038,  59944,  ...,  96859, 158981, 169342]),\n",
       "             val=tensor([0.7762, 0.0135, 0.0122,  ..., 0.0255, 0.3191, 0.2222]),\n",
       "             size=(169343, 169343), nnz=2723277, density=0.01%), x_hat=None, logits=None, logits_features=None, latent=tensor([[ 0.0146,  0.0698, -0.1315,  ..., -0.0511, -0.1003,  0.1225],\n",
       "        [ 0.1629, -0.0010,  0.0820,  ..., -0.0905, -0.1064,  0.0186],\n",
       "        [ 0.0720, -0.0055, -0.0464,  ..., -0.0859, -0.0278, -0.0082],\n",
       "        ...,\n",
       "        [-0.0531, -0.0876,  0.0416,  ...,  0.0261,  0.0063,  0.0457],\n",
       "        [ 0.0481,  0.0169, -0.0269,  ..., -0.0800, -0.0707,  0.0909],\n",
       "        [ 0.2491,  0.1819, -0.3114,  ..., -0.0774,  0.0026, -0.0010]]), latent_node=None, latent_features=tensor([[ 0.0146,  0.0698, -0.1315,  ..., -0.0511, -0.1003,  0.1225],\n",
       "        [ 0.1629, -0.0010,  0.0820,  ..., -0.0905, -0.1064,  0.0186],\n",
       "        [ 0.0720, -0.0055, -0.0464,  ..., -0.0859, -0.0278, -0.0082],\n",
       "        ...,\n",
       "        [-0.0531, -0.0876,  0.0416,  ...,  0.0261,  0.0063,  0.0457],\n",
       "        [ 0.0481,  0.0169, -0.0269,  ..., -0.0800, -0.0707,  0.0909],\n",
       "        [ 0.2491,  0.1819, -0.3114,  ..., -0.0774,  0.0026, -0.0010]]), hidden=tensor([[0.4267, 0.0330, 0.0000,  ..., 0.1862, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0700, 0.0000, 0.0000,  ..., 0.0987, 0.0000, 0.0634],\n",
       "        ...,\n",
       "        [0.0837, 0.0676, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3752, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],\n",
       "        [0.1621, 0.0000, 0.0000,  ..., 0.4673, 0.0000, 0.0000]]), hidden_features=tensor([[0.4267, 0.0330, 0.0000,  ..., 0.1862, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0700, 0.0000, 0.0000,  ..., 0.0987, 0.0000, 0.0634],\n",
       "        ...,\n",
       "        [0.0837, 0.0676, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3752, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],\n",
       "        [0.1621, 0.0000, 0.0000,  ..., 0.4673, 0.0000, 0.0000]]), var_predicted=None, var=None, softs=None, energy=None, energy_feaures=None, p_c=None, p_u=None, p_uc=None, chi=None, evidence=tensor([[ 9729.3281],\n",
       "        [14745.5352],\n",
       "        [ 9450.3203],\n",
       "        ...,\n",
       "        [11008.9746],\n",
       "        [10572.5801],\n",
       "        [13348.4492]]), evidence_ft=tensor([ 9203.0605,  9049.6455,  5164.0967,  ...,  9245.5527,  6546.1890,\n",
       "        22053.8418]), evidence_nn=None, evidence_node=None, evidence_per_class=None, evidence_ft_per_class=None, ft_weight=None, nn_weight=None, log_ft=None, log_ft_per_class=tensor([[-18.1703, -16.9401, -15.1372,  ..., -14.7463, -19.3704, -17.0111],\n",
       "        [-16.3466, -18.2904, -15.1575,  ..., -15.6051, -18.2983, -14.6490],\n",
       "        [-16.7271, -16.6574, -16.1545,  ..., -15.4514, -17.8734, -15.8904],\n",
       "        ...,\n",
       "        [-17.9309, -17.7658, -14.8843,  ..., -15.7090, -17.3768, -17.2972],\n",
       "        [-18.1809, -17.4603, -14.9730,  ..., -15.4284, -19.7665, -16.6049],\n",
       "        [-18.9642, -14.3984, -16.3463,  ..., -14.7698, -17.8234, -17.4262]]), log_nn=None, log_nn_per_class=None, log_node=None, prediction_confidence_aleatoric=tensor([0.3434, 0.4361, 0.4080,  ..., 0.2437, 0.2019, 0.4793]), prediction_confidence_epistemic=tensor([3258.5999, 8635.1875, 4977.8286,  ..., 3080.4431, 4455.8511,\n",
       "        6588.4492]), prediction_confidence_structure=None, sample_confidence_total=tensor([0.3434, 0.4361, 0.4080,  ..., 0.2437, 0.2019, 0.4793]), sample_confidence_total_entropy=tensor([-2.6345, -2.2541, -2.4566,  ..., -2.5333, -2.7752, -2.1102]), sample_confidence_aleatoric=tensor([0.3434, 0.4361, 0.4080,  ..., 0.2437, 0.2019, 0.4793]), sample_confidence_aleatoric_entropy=tensor([-2.3891, -1.9910, -2.1478,  ..., -2.2385, -2.2826, -1.9910]), sample_confidence_epistemic=tensor([ 9769.3291, 14785.5361,  9490.3213,  ..., 11048.9746, 10612.5801,\n",
       "        13388.4473]), sample_confidence_epistemic_entropy=tensor([226.4146, 240.0626, 224.8981,  ..., 229.2888, 223.2633, 240.3971]), sample_confidence_epistemic_entropy_diff=tensor([-0.2454, -0.2631, -0.3088,  ..., -0.2948, -0.4926, -0.1192]), sample_confidence_structure=None, sample_confidence_features=tensor([ 9243.0605,  9089.6455,  5204.0967,  ...,  9285.5527,  6586.1890,\n",
       "        22093.8418]), sample_confidence_neighborhood=None, mu_1=None, mu_1p=None, mu_2=None, mu_2p=None, var_1=None, var_1p=None, var_2=None, var_2p=None, log_q=None, log_prior=None, act_vec=None, q=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import train_and_eval as tae\n",
    "\n",
    "def get_config_name(model):\n",
    "    if model in (\"gpn\", \"gpn_rw\", \"gpn_lop\"):\n",
    "        return \"configs/gpn/classification_gpn_16.yaml\"\n",
    "    return f\"configs/reference/classification_{model}.yaml\"\n",
    "\n",
    "def get_config_updates(model, dataset):\n",
    "    updates = {}\n",
    "    match model:\n",
    "        case \"gpn\":\n",
    "            model_name = \"GPN\"\n",
    "        case \"gpn_rw\":\n",
    "            model_name = \"GPN\"\n",
    "            updates[\"model.adj_normalization\"] = \"rw\"\n",
    "        case \"gpn_lop\":\n",
    "            model_name = \"GPN_LOP\"\n",
    "            updates[\"model.sparse_x_prune_threshold\"] = 0.01\n",
    "        case _:\n",
    "            model_name = model.upper()\n",
    "\n",
    "    if dataset in (\"AmazonPhotos\", \"AmazonComputers\", \"PubMedFull\"):\n",
    "        updates[\"model.sparse_propagation\"] = True\n",
    "\n",
    "    if dataset == \"ogbn-arxiv\":\n",
    "        updates[\"model.sparse_propagation\"] = True\n",
    "        updates[\"data.split\"] = \"public\"\n",
    "        updates[\"model.entropy_num_samples\"] = 100\n",
    "\n",
    "    return {\n",
    "        \"model.model_name\": model_name,\n",
    "        \"data.dataset\": dataset,\n",
    "        \"run.num_inits\": 1,\n",
    "        \"run.num_splits\": 1,\n",
    "        \"run.log\": False,\n",
    "        \"run.job\": \"predict\",\n",
    "        **updates\n",
    "    }\n",
    "\n",
    "def get_prediction(model, dataset):\n",
    "    res = tae.ex.run(\n",
    "        named_configs=[get_config_name(model)],\n",
    "        config_updates={\n",
    "            **get_config_updates(model, dataset),\n",
    "            # \"run.reduced_training_metrics\": True,\n",
    "            # \"training.eval_every\": 10,\n",
    "            # \"training.stopping_patience\": 5,\n",
    "            # \"data.split\": \"public\",\n",
    "            # \"run.num_splits\": 1,\n",
    "            # \"model.model_name\": \"GPN_LOP\",\n",
    "            # \"model.sparse_x_prune_threshold\": 0.01,\n",
    "            # \"run.reeval\": True,\n",
    "        },\n",
    "        options={\"--force\": True},\n",
    "    )\n",
    "\n",
    "    assert isinstance(res.result, list) and len(res.result) == 1\n",
    "    return res.result[0]\n",
    "\n",
    "gpn_rw_pred = get_prediction(\"gpn_rw\", dataset)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gpn_lop_pred = get_prediction(\"gpn_lop\", dataset)\n",
    "\n",
    "gpn_lop_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def generate_graph(data, preds, name):\n",
    "    edges = data.edge_index.T.numpy()\n",
    "\n",
    "    y = data.y.numpy()\n",
    "\n",
    "    nodes = {\n",
    "        \"true_y\": y,\n",
    "        \"train_mask\": data.train_mask.numpy().astype(int),\n",
    "        \"val_mask\": data.val_mask.numpy().astype(int),\n",
    "        \"test_mask\": data.test_mask.numpy().astype(int),\n",
    "    }\n",
    "\n",
    "    for model, pred in preds.items():\n",
    "        y_pred = pred.hard.numpy()\n",
    "        err = (y != y_pred).astype(int)\n",
    "        pc_aleatoric = pred.prediction_confidence_aleatoric.numpy()\n",
    "        pc_epistemic = pred.prediction_confidence_epistemic.numpy()\n",
    "        sc_total = pred.sample_confidence_total.numpy()\n",
    "        sc_total_entropy = pred.sample_confidence_total_entropy.numpy()\n",
    "        sc_aleatoric = pred.sample_confidence_aleatoric.numpy()\n",
    "        sc_aleatoric_entropy = pred.sample_confidence_aleatoric_entropy.numpy()\n",
    "        sc_epistemic = pred.sample_confidence_epistemic.numpy()\n",
    "        sc_epistemic_entropy = pred.sample_confidence_epistemic_entropy.numpy()\n",
    "        sc_epistemic_entropy_diff = pred.sample_confidence_epistemic_entropy_diff.numpy()\n",
    "        nodes[f\"{model}_y\"] = y_pred\n",
    "        nodes[f\"{model}_err\"] = err\n",
    "        nodes[f\"{model}_pc_aleatoric\"] = pc_aleatoric\n",
    "        nodes[f\"{model}_pc_epistemic\"] = pc_epistemic\n",
    "        nodes[f\"{model}_sc_total\"] = sc_total\n",
    "        nodes[f\"{model}_sc_total_entropy\"] = sc_total_entropy\n",
    "        nodes[f\"{model}_sc_aleatoric\"] = sc_aleatoric\n",
    "        nodes[f\"{model}_sc_aleatoric_entropy\"] = sc_aleatoric_entropy\n",
    "        nodes[f\"{model}_sc_epistemic\"] = sc_epistemic\n",
    "        nodes[f\"{model}_sc_epistemic_entropy\"] = sc_epistemic_entropy\n",
    "        nodes[f\"{model}_sc_epistemic_entropy_diff\"] = sc_epistemic_entropy_diff\n",
    "\n",
    "    g = nx.Graph()\n",
    "\n",
    "    g.add_nodes_from(range(len(y)))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        node = g.nodes[i]\n",
    "        for k, v in nodes.items():\n",
    "            node[k] = v[i]\n",
    "\n",
    "    g.add_edges_from(edges)\n",
    "\n",
    "    nx.write_graphml(g, f\"{name}.graphml\")\n",
    "\n",
    "    return g\n",
    "\n",
    "g = generate_graph(data, {\n",
    "    \"gpn_rw\": gpn_rw_pred,\n",
    "    \"gpn_lop\": gpn_lop_pred\n",
    "}, f\"graphs/{dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8253, 0.8322, 0.8284, 0.8317, 0.8232, 0.8375, 0.8340, 0.8314, 0.8300,\n",
       "        0.8359], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def simplex_samples(alpha: torch.Tensor, num_samples=1000):\n",
    "    dirichlet = torch.distributions.Dirichlet(alpha)\n",
    "    samples = dirichlet.sample((num_samples,))\n",
    "    entropies = torch.distributions.Categorical(samples).entropy()\n",
    "    exp_entropies = entropies.mean(axis=0)\n",
    "\n",
    "    return exp_entropies\n",
    "\n",
    "_ = simplex_samples(torch.tensor([[1.0]], device=0).repeat(10, 3))\n",
    "\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
